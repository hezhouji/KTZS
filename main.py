import akshare as ak
import pandas as pd
import numpy as np
from scipy import stats
from scipy.optimize import minimize
import requests
import os
from datetime import datetime, timedelta

# --- åŸºç¡€é…ç½® ---
FEISHU_WEBHOOK = os.getenv("FEISHU_WEBHOOK")
DATA_DIR = "KTZS"
LOG_FILE = "HISTORY_LOG.csv"

def log(msg):
    print(f"[{datetime.now().strftime('%H:%M:%S')}] {msg}")

def normalize_date(d_val):
    """å¼ºè¡Œå°†å„ç§ä¹±åºæ—¥æœŸè½¬ä¸º YYYY-MM-DD"""
    s = str(d_val).replace(".txt", "").replace("å¹´", "-").replace("æœˆ", "-").replace("æ—¥", "")
    for fmt in ("%Y-%m-%d", "%Y%m%d"):
        try: return datetime.strptime(s, fmt).date()
        except: continue
    return None

def get_actual(date_obj):
    target = date_obj.strftime("%Y%m%d")
    if not os.path.exists(DATA_DIR): return None
    for f in os.listdir(DATA_DIR):
        if target in f and f.endswith(".txt"):
            try:
                with open(os.path.join(DATA_DIR, f), "r") as file:
                    return float(file.read().strip())
            except: pass
    return None

def calculate_factors(target_date, df_p_all, df_val_all, df_bond_all):
    """å¸¦æ—¶é—´åˆ‡ç‰‡çš„è®¡ç®—é€»è¾‘"""
    try:
        # åªå–ç›®æ ‡æ—¥æœŸä¹‹å‰çš„æ•°æ®ï¼Œç¡®ä¿æ¯ä¸€å¤©çš„ f1-f6 éƒ½æœ‰æ³¢åŠ¨
        df_p = df_p_all[df_p_all['date'] <= target_date].copy()
        df_val = df_val_all[df_val_all['date_key'] <= target_date].copy()
        df_bond = df_bond_all[df_bond_all['date_key'] <= target_date].copy()

        if len(df_p) < 30: return [50.0]*6

        def p_score(series, cur, inv=False):
            p = stats.percentileofscore(series.dropna(), cur)
            return float(100 - p if inv else p)

        # æ³¢åŠ¨ç‡
        v = df_p['close'].pct_change().rolling(20).std()
        f1 = p_score(v, v.iloc[-1], inv=True)
        # æˆäº¤é‡
        v20 = df_p['volume'].rolling(20).mean()
        f2 = p_score(df_p['volume']/v20, (df_p['volume']/v20).iloc[-1])
        # å¼ºåº¦
        h250 = df_p['close'].rolling(250).max()
        f3 = p_score(df_p['close']/h250, (df_p['close']/h250).iloc[-1])
        # åŸºå·® (æ¨¡æ‹Ÿ)
        f4 = 50.0
        # é¿é™© (ERP)
        pe_col = 'å¸‚ç›ˆç‡1' if 'å¸‚ç›ˆç‡1' in df_val.columns else 'å¸‚ç›ˆç‡TTM'
        erp = (1/df_val[pe_col].astype(float)) - (df_bond['ä¸­å›½å›½å€ºæ”¶ç›Šç‡10å¹´'].astype(float)/100)
        f5 = p_score(erp, erp.iloc[-1], inv=True)
        # æ æ† (æ¨¡æ‹Ÿ)
        f6 = 50.0

        return [round(x, 2) for x in [f1, f2, f3, f4, f5, f6]]
    except: return [50.0]*6

def main():
    log("=== å¯åŠ¨ AI è‡ªé€‚åº”é¢„æµ‹ç³»ç»Ÿ ===")
    today = datetime.now().date()
    cols = ["date","f1","f2","f3","f4","f5","f6","predict","actual","bias"]

    # åˆå§‹åŒ–æˆ–ä¿®å¤ç©º CSV
    if os.path.exists(LOG_FILE):
        try:
            df_log = pd.read_csv(LOG_FILE)
            if df_log.empty: df_log = pd.DataFrame(columns=cols)
        except: df_log = pd.DataFrame(columns=cols)
    else:
        df_log = pd.DataFrame(columns=cols)

    # ç»Ÿä¸€æ¸…æ´—å†å²æ—¥æœŸæ ¼å¼
    if not df_log.empty:
        df_log['date'] = df_log['date'].apply(lambda x: normalize_date(x).strftime("%Y-%m-%d") if normalize_date(x) else x)

    # è·å–å…¨é‡å¸‚åœºæ•°æ®
    df_p = ak.stock_zh_index_daily(symbol="sh000300")
    df_p['date'] = pd.to_datetime(df_p['date']).dt.date
    df_val = ak.stock_zh_index_value_csindex(symbol="000300")
    df_val['date_key'] = pd.to_datetime(df_val['æ—¥æœŸ']).dt.date
    df_bond = ak.bond_zh_us_rate()
    df_bond['date_key'] = pd.to_datetime(df_bond['æ—¥æœŸ']).dt.date

    # å†å²è¡¥å…¨ (è¿‡å»14å¤©)
    for i in range(14, 0, -1):
        d = today - timedelta(days=i)
        act = get_actual(d)
        if act:
            d_str = d.strftime("%Y-%m-%d")
            fs = calculate_factors(d, df_p, df_val, df_bond)
            df_log = df_log[df_log['date'] != d_str] # è¦†ç›–æ—§æ ¼å¼
            df_log.loc[len(df_log)] = [d_str] + fs + [round(sum(fs)/6, 2), act, round(act-sum(fs)/6, 2)]

    # åŠ¨æ€æƒé‡ä¼˜åŒ–
    weights = np.array([1/6]*6)
    df_fit = df_log.dropna(subset=['actual']).tail(7)
    if len(df_fit) >= 7:
        X, y = df_fit[['f1','f2','f3','f4','f5','f6']].values, df_fit['actual'].values
        res = minimize(lambda w: np.sum((X@w-y)**2), weights, bounds=[(0.05, 0.4)]*6, constraints={'type':'eq','fun':lambda w: sum(w)-1})
        if res.success: weights = res.x

    # ä»Šæ—¥é¢„æµ‹
    tf = calculate_factors(today, df_p, df_val, df_bond)
    tp = round(sum(f*w for f, w in zip(tf, weights)), 2)
    t_str = today.strftime("%Y-%m-%d")
    df_log = df_log[df_log['date'] != t_str]
    df_log.loc[len(df_log)] = [t_str] + tf + [tp, np.nan, np.nan]

    df_log.sort_values('date').to_csv(LOG_FILE, index=False)
    
    # é£ä¹¦æ¨é€
# åœ¨ main.py çš„ main() å‡½æ•°æœ«å°¾æ›¿æ¢è¿™æ®µæ¨é€é€»è¾‘
    # ... ä¹‹å‰çš„ä»£ç ä¿æŒä¸å˜ ...

    # æ„å»ºæ¨é€å†…å®¹
    w_info = " | ".join([f"{n}:{w:.0%}" for n, w in zip(["æ³¢åŠ¨","é‡èƒ½","å¼ºåº¦","æœŸè´§","é¿é™©","æ æ†"], weights)])
    payload = {
        "msg_type": "interactive",
        "card": {
            "header": {"title": {"tag": "plain_text", "content": f"ğŸ¯ æè´ª AI é¢„æµ‹ ({today})"}, "template": "purple"},
            "elements": [
                {"tag": "div", "text": {"tag": "lark_md", "content": f"**ä»Šæ—¥é¢„æµ‹å€¼ï¼š{tp}**\n\nğŸ“Š **æƒé‡å¯¹é½ç°çŠ¶ï¼š**\n{w_info}"}},
                {"tag": "note", "elements": [{"tag": "plain_text", "content": f"åŸå§‹ç»´åº¦åˆ†: {' / '.join(map(str, tf))}"}]}
            ]
        }
    }
    
    if FEISHU_WEBHOOK:
        response = requests.post(FEISHU_WEBHOOK, json=payload)
        print(f"é£ä¹¦æ¨é€çŠ¶æ€ç : {response.status_code}")
        print(f"é£ä¹¦è¿”å›è¯¦æƒ…: {response.text}")
    else:
        print("é”™è¯¯: æœªæ£€æµ‹åˆ° FEISHU_WEBHOOK ç¯å¢ƒå˜é‡")

if __name__ == "__main__":
    main()