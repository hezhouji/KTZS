import akshare as ak
import pandas as pd
import numpy as np
from scipy import stats
import requests
import os
from datetime import datetime, timedelta

# é£ä¹¦é…ç½®
FEISHU_WEBHOOK = os.getenv("FEISHU_WEBHOOK")
DATA_DIR = "KTZS"

def log(msg):
    print(f"[{datetime.now().strftime('%H:%M:%S')}] {msg}")

def get_actual_val(date_str):
    path = os.path.join(DATA_DIR, f"{date_str}.txt")
    if os.path.exists(path):
        try:
            with open(path, "r") as f:
                val = float(f.read().strip())
                log(f"æˆåŠŸè¯»å–æ˜¨æ—¥å®é™…å€¼: {val}")
                return val
        except Exception as e:
            log(f"è¯»å–æ–‡ä»¶å†…å®¹å¤±è´¥: {e}")
    else:
        log(f"æœªæ‰¾åˆ°æ˜¨æ—¥æ ¡å‡†æ–‡ä»¶: {path}")
    return None

def get_p_score(series, current_val, reverse=False):
    series = series.dropna()
    if series.empty or np.isnan(current_val): return 50
    p = stats.percentileofscore(series, current_val, kind='weak')
    return 100 - p if reverse else p

def analyze_factors(target_date, df_p, df_val, df_bond):
    log(f"æ­£åœ¨åˆ†ææ—¥æœŸ {target_date} çš„å¤šç»´åº¦å› å­...")
    try:
        df_curr = df_p[df_p['date'] <= target_date].copy()
        if df_curr.empty: return None

        # 1. åŠ¨èƒ½ (250æ—¥ä½ç½®)
        h250 = df_curr['close'].rolling(250).max()
        s_score = get_p_score(df_curr['close']/h250, (df_curr['close']/h250).iloc[-1])
        
        # 2. é‡èƒ½ (20æ—¥å‡é‡æ¯”)
        v20 = df_curr['volume'].rolling(20).mean()
        v_score = get_p_score(df_curr['volume']/v20, (df_curr['volume']/v20).iloc[-1])
        
        # 3. è‚¡å€º (ERP)
        pe_col = 'å¸‚ç›ˆç‡1' if 'å¸‚ç›ˆç‡1' in df_val.columns else 'å¸‚ç›ˆç‡TTM'
        # ç®€å•åŒ¹é…å½“æ—¥ERP
        merged = pd.merge(df_val[['date_key', pe_col]], df_bond[['date_key', 'ä¸­å›½å›½å€ºæ”¶ç›Šç‡10å¹´']], on='date_key')
        merged = merged[merged['date_key'] <= target_date]
        if not merged.empty:
            merged['erp'] = (1 / merged[pe_col].astype(float)) - (merged['ä¸­å›½å›½å€ºæ”¶ç›Šç‡10å¹´'].astype(float) / 100)
            e_score = get_p_score(merged['erp'], merged['erp'].iloc[-1], reverse=True)
        else: e_score = 50

        # 4. æƒ…ç»ªä¹–ç¦»
        bias = (df_curr['close'] - df_curr['close'].rolling(20).mean()) / df_curr['close'].rolling(20).mean()
        b_score = get_p_score(bias, bias.iloc[-1])

        raw = (s_score * 0.4) + (v_score * 0.3) + (e_score * 0.15) + (b_score * 0.15)
        return {"score": raw, "s": s_score, "v": v_score, "e": e_score, "b": b_score}
    except Exception as e:
        log(f"å› å­è®¡ç®—å¼‚å¸¸: {e}")
        return None

def send_feishu(content):
    if not FEISHU_WEBHOOK:
        log("é”™è¯¯: æœªé…ç½®é£ä¹¦ Webhook ç¯å¢ƒå˜é‡")
        return
    
    log("æ­£åœ¨å‘é€é£ä¹¦é€šçŸ¥...")
    # æ³¨æ„ï¼šæ ‡é¢˜å¿…é¡»åŒ…å«ä½ åœ¨é£ä¹¦æœºå™¨äººåå°è®¾ç½®çš„â€œå…³é”®è¯â€
    payload = {
        "msg_type": "interactive",
        "card": {
            "header": {"title": {"tag": "plain_text", "content": f"ğŸ“Š æè´ªæŒ‡æ•°é¢„æµ‹åŒæ­¥ ({content['date']})"}, "template": "orange"},
            "elements": [
                {"tag": "div", "text": {"tag": "lark_md", "content": f"**ä»Šæ—¥æ¨æµ‹æ•°å€¼ï¼š{content['final']}**\nå…¬å¼ï¼šæ¨¡å‹({content['raw']}) + ä¿®æ­£({content['bias']})"}},
                {"tag": "hr"},
                {"tag": "div", "text": {"tag": "lark_md", "content": f"**å½’å› é€»è¾‘ï¼š**\n{content['reason']}"}},
                {"tag": "note", "elements": [{"tag": "plain_text", "content": "æ³¨ï¼šè¯¥é¢„æµ‹å·²æ ¹æ®æ˜¨æ—¥éŸ­åœˆå„¿å®é™…è¯¯å·®è‡ªåŠ¨æ ¡å‡†ã€‚"}]}
            ]
        }
    }
    
    try:
        res = requests.post(FEISHU_WEBHOOK, json=payload, timeout=10)
        log(f"é£ä¹¦è¿”å›çŠ¶æ€ç : {res.status_code}")
        log(f"é£ä¹¦è¿”å›å†…å®¹: {res.text}")
        if res.status_code != 200:
            log("æç¤º: è¯·æ£€æŸ¥é£ä¹¦æœºå™¨äººå®‰å…¨è®¾ç½®ä¸­çš„'å…³é”®è¯'æ˜¯å¦åŒ…å«ã€æè´ªã€‘æˆ–ã€æŒ‡æ•°ã€‘")
    except Exception as e:
        log(f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")

def main():
    log("=== å¯åŠ¨è‡ªåŠ¨åŒ–åˆ†ææµç¨‹ ===")
    today = datetime.now().date()
    yest = today - timedelta(days=1)
    
    log(f"ä»Šæ—¥æ—¥æœŸ: {today}, ç›®æ ‡å¯¹æ ‡æ—¥æœŸ: {yest}")

    # 1. æŠ“å–æ•°æ®
    log("å¼€å§‹æ‹‰å– akshare æ•°æ®...")
    df_p = ak.stock_zh_index_daily(symbol="sh000300")
    df_p['date'] = pd.to_datetime(df_p['date']).dt.date
    df_val = ak.stock_zh_index_value_csindex(symbol="000300")
    df_val['date_key'] = pd.to_datetime(df_val['æ—¥æœŸ']).dt.date
    df_bond = ak.bond_zh_us_rate()
    df_bond['date_key'] = pd.to_datetime(df_bond['æ—¥æœŸ']).dt.date
    log("æ•°æ®æ‹‰å–å®Œæ¯•")

    # 2. è®¡ç®—æ˜¨æ—¥æ¨¡å‹ä¸è·å–å®é™…å€¼
    yest_model = analyze_factors(yest, df_p, df_val, df_bond)
    yest_actual = get_actual_val(yest.strftime("%Y%m%d"))
    
    # 3. è®¡ç®—åå·®
    bias = 0
    reason = "ç»§æ‰¿æ˜¨æ—¥è¯¯å·®æƒ¯æ€§"
    if yest_model and yest_actual:
        bias = yest_actual - yest_model['score']
        log(f"è®¡ç®—å¾—å‡ºåå·®: {bias:+.2f}")
    else:
        log("è­¦å‘Š: ç¼ºå°‘æ˜¨æ—¥å¯¹æ¯”æ•°æ®ï¼Œä¿®æ­£å€¼ä¸º0")

    # 4. è®¡ç®—ä»Šæ—¥é¢„æµ‹
    today_model = analyze_factors(today, df_p, df_val, df_bond)
    if today_model:
        # ç®€å•ç¯å¢ƒä¿®æ­£
        vol_change = (df_p['volume'].iloc[-1] / df_p['volume'].iloc[-2]) - 1
        if vol_change > 0.2: 
            bias *= 1.1
            reason = "ä»Šæ—¥æ”¾é‡æ˜¾è‘—ï¼Œå¼ºåŒ–äº¢å¥‹åç½®"
        
        final_val = round(max(0, min(100, today_model['score'] + bias)), 2)
        log(f"ä»Šæ—¥æœ€ç»ˆæ¨æµ‹ç»“æœ: {final_val}")
        
        # 5. å‘é€
        send_data = {
            "date": today.strftime("%Y-%m-%d"),
            "final": final_val,
            "raw": round(today_model['score'], 2),
            "bias": round(bias, 2),
            "reason": reason
        }
        send_feishu(send_data)
    
    log("=== æµç¨‹ç»“æŸ ===")

if __name__ == "__main__":
    main()