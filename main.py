import akshare as ak
import pandas as pd
import numpy as np
from scipy import stats
from scipy.optimize import minimize
import requests
import os
import time
from datetime import datetime, timedelta

# --- åŸºç¡€é…ç½® ---
FEISHU_WEBHOOK = os.getenv("FEISHU_WEBHOOK")
DATA_DIR = "KTZS"
LOG_FILE = "HISTORY_LOG.csv"

def log(msg):
    print(f"[{datetime.now().strftime('%H:%M:%S')}] {msg}")

def is_workday(d_obj):
    return d_obj.weekday() < 5

def normalize_date(d_val):
    if not d_val or pd.isna(d_val): return None
    s = str(d_val).replace(".txt", "").replace("å¹´", "-").replace("æœˆ", "-").replace("æ—¥", "")
    for fmt in ("%Y-%m-%d", "%Y%m%d"):
        try: return datetime.strptime(s, fmt).date()
        except: continue
    return None

def get_actual_val(date_obj):
    target = date_obj.strftime("%Y%m%d")
    if not os.path.exists(DATA_DIR): return None
    for f in os.listdir(DATA_DIR):
        if target in f and f.endswith(".txt"):
            try:
                with open(os.path.join(DATA_DIR, f), "r") as file:
                    return float(file.read().strip())
            except: pass
    return None

# --- æ•°æ®è·å–æ¨¡å— (å¢åŠ é‡è¯•æœºåˆ¶) ---
def fetch_data_with_retry(func, **kwargs):
    for _ in range(3):
        try:
            return func(**kwargs)
        except:
            time.sleep(2)
    return pd.DataFrame()

def main():
    log("=== å¯åŠ¨å…¨é‡çœŸå®æ•°æ® AI ç³»ç»Ÿ (å»ä¼ªæ±‚çœŸç‰ˆ) ===")
    
    # 0. å¼ºåŠ›æ¸…ç†æ—§æ•°æ®
    if os.path.exists(LOG_FILE):
        os.remove(LOG_FILE)
        log("å·²åˆ é™¤æ—§è¡¨æ ¼ï¼Œå‡†å¤‡é‡æ–°è®¡ç®—")

    today = datetime.now().date()
    
    # --- 1. è·å–å…¨ç»´åº¦çœŸå®æ•°æ® ---
    log("1/5 æ­£åœ¨è·å– æ²ªæ·±300 ç°è´§æ•°æ®...")
    df_p = fetch_data_with_retry(ak.stock_zh_index_daily, symbol="sh000300")
    df_p['date'] = pd.to_datetime(df_p['date']).dt.date
    
    log("2/5 æ­£åœ¨è·å– è‚¡æŒ‡æœŸè´§(IF) æ•°æ® (è®¡ç®—å‡è´´æ°´)...")
    # IF0 ä»£è¡¨æ²ªæ·±300ä¸»åŠ›è¿ç»­åˆçº¦
    df_fut = fetch_data_with_retry(ak.futures_zh_daily_sina, symbol="IF0")
    if not df_fut.empty:
        df_fut['date'] = pd.to_datetime(df_fut['date']).dt.date
    else:
        log("âš ï¸ è­¦å‘Š: æœŸè´§æ•°æ®è·å–å¤±è´¥ï¼Œf4 å°†å—å½±å“")

    log("3/5 æ­£åœ¨è·å– ä¼°å€¼ä¸å›½å€ºæ•°æ® (è®¡ç®—ERP)...")
    df_val = fetch_data_with_retry(ak.stock_zh_index_value_csindex, symbol="000300")
    df_val['date_key'] = pd.to_datetime(df_val['æ—¥æœŸ']).dt.date
    
    df_bond = fetch_data_with_retry(ak.bond_zh_us_rate)
    df_bond['date_key'] = pd.to_datetime(df_bond['æ—¥æœŸ']).dt.date

    log("4/5 æ­£åœ¨è·å– èèµ„èåˆ¸æ•°æ® (è®¡ç®—æ æ†)...")
    # äº¤æ˜“æ‰€èèµ„æ•°æ®é€šå¸¸å»¶è¿Ÿä¸€å¤©
    df_margin = fetch_data_with_retry(ak.stock_margin_account_exchange)
    if not df_margin.empty:
        df_margin['date_key'] = pd.to_datetime(df_margin['æ—¥æœŸ']).dt.date

    # --- 2. å®šä¹‰å› å­è®¡ç®—å¼•æ“ ---
    def calculate_factors(target_date, _df_p, _df_fut, _df_val, _df_bond, _df_margin):
        try:
            # æ•°æ®åˆ‡ç‰‡ï¼šç»ä¸ä½¿ç”¨æœªæ¥çš„æ•°æ®
            cut_p = _df_p[_df_p['date'] <= target_date].copy()
            
            if len(cut_p) < 30: return [50.0] * 6 # æ•°æ®ä¸è¶³

            # é€šç”¨åˆ†ä½æ•°è®¡ç®—å‡½æ•°
            def get_score(series, current_val, invert=False):
                # æ¸…æ´—æ•°æ®ï¼Œç¡®ä¿æ˜¯æ•°å­—
                s = pd.to_numeric(series, errors='coerce').dropna()
                if len(s) < 10 or pd.isna(current_val): return 50.0
                
                # è®¡ç®—å½“å‰å€¼åœ¨å†å²(è¿‡å»3å¹´/750å¤©)ä¸­çš„ç™¾åˆ†ä½
                # ä½¿ç”¨è¿‘3å¹´çª—å£æ›´ç¬¦åˆå¸‚åœºè¿‘å†µ
                s_window = s.tail(750) 
                p = stats.percentileofscore(s_window, current_val, kind='weak')
                return float(100 - p if invert else p)

            # [f1] æ³¢åŠ¨ç‡ (20æ—¥) - åå‘
            vol = cut_p['close'].pct_change().rolling(20).std()
            f1 = get_score(vol, vol.iloc[-1], invert=True)

            # [f2] æˆäº¤é‡ (20æ—¥å‡æ¯”) - æ­£å‘
            vol_ratio = cut_p['volume'] / cut_p['volume'].rolling(20).mean()
            f2 = get_score(vol_ratio, vol_ratio.iloc[-1], invert=False)

            # [f3] ä»·æ ¼å¼ºåº¦ (250æ—¥é«˜ç‚¹æ¯”) - æ­£å‘
            high_250 = cut_p['close'].rolling(250).max()
            strength = cut_p['close'] / high_250
            f3 = get_score(strength, strength.iloc[-1], invert=False)

            # [f4] å‡è´´æ°´ç‡ (æœŸè´§ - ç°è´§) / ç°è´§ - æ­£å‘
            # é€»è¾‘ï¼šå‡æ°´(æœŸè´§>ç°è´§)ä»£è¡¨çœ‹å¤šï¼Œè´´æ°´ä»£è¡¨çœ‹ç©º
            f4 = 50.0
            if not _df_fut.empty:
                cut_f = _df_fut[_df_fut['date'] <= target_date].copy()
                # åˆå¹¶ç°è´§å’ŒæœŸè´§
                merged = pd.merge(cut_p[['date','close']], cut_f[['date','close']], on='date', suffixes=('_spot', '_fut'))
                if not merged.empty:
                    merged['basis_rate'] = (merged['close_fut'] - merged['close_spot']) / merged['close_spot']
                    f4 = get_score(merged['basis_rate'], merged['basis_rate'].iloc[-1], invert=False)

            # [f5] è‚¡å€ºæ€§ä»·æ¯” ERP (1/PE - å›½å€º) - æ­£å‘
            # é€»è¾‘ï¼šERPè¶Šé«˜ï¼Œè‚¡ç¥¨è¶Šæœ‰å¸å¼•åŠ›ï¼Œåº”è¯¥æ˜¯è´ªå©ª(é«˜åˆ†)
            f5 = 50.0
            # æ™ºèƒ½åˆ—ååŒ¹é…
            pe_col = next((c for c in _df_val.columns if 'å¸‚ç›ˆç‡' in c and 'TTM' in c), None) # ä¼˜å…ˆæ‰¾TTM
            if not pe_col: pe_col = next((c for c in _df_val.columns if 'å¸‚ç›ˆç‡' in c), None)
            
            rate_col = next((c for c in _df_bond.columns if '10å¹´' in c), None)

            if pe_col and rate_col:
                # å¿…é¡»å¯¹é½æ—¥æœŸ
                cut_v = _df_val[_df_val['date_key'] <= target_date].set_index('date_key')[[pe_col]]
                cut_b = _df_bond[_df_bond['date_key'] <= target_date].set_index('date_key')[[rate_col]]
                
                # åˆå¹¶
                erp_df = cut_v.join(cut_b).dropna()
                if not erp_df.empty:
                    erp_series = (1 / pd.to_numeric(erp_df[pe_col])) - (pd.to_numeric(erp_df[rate_col]) / 100)
                    # ERP è¶Šé«˜è¶Šå€¼å¾—ä¹°(è´ªå©ª)ï¼Œæ‰€ä»¥ä¸åå‘
                    f5 = get_score(erp_series, erp_series.iloc[-1], invert=False)

            # [f6] æ æ†èµ„é‡‘ (èèµ„ä½™é¢) - æ­£å‘
            # é€»è¾‘ï¼šèèµ„ä½™é¢é«˜ä»£è¡¨æ•£æˆ·ç‹‚çƒ­
            f6 = 50.0
            if not _df_margin.empty:
                cut_m = _df_margin[_df_margin['date_key'] <= target_date].copy()
                if not cut_m.empty:
                    # èèµ„ä½™é¢åˆ—åé€šå¸¸æ˜¯ "èèµ„ä½™é¢" æˆ– "rzye"
                    margin_col = next((c for c in cut_m.columns if 'èèµ„ä½™é¢' in c), None)
                    if margin_col:
                        m_val = pd.to_numeric(cut_m[margin_col], errors='coerce')
                        # æ æ†æ•°æ®é€šå¸¸æ»åï¼Œå–æœ€è¿‘çš„ä¸€ä¸ªæœ‰æ•ˆå€¼
                        f6 = get_score(m_val, m_val.iloc[-1], invert=False)

            return [round(x, 2) for x in [f1, f2, f3, f4, f5, f6]]
        except Exception as e:
            log(f"æ—¥æœŸ {target_date} è®¡ç®—å‡ºé”™: {e}")
            return [50.0] * 6

    # --- 3. é‡å»ºå†å²ä¸é¢„æµ‹ ---
    log("5/5 å¼€å§‹å›æº¯è®¡ç®—å†å² (æœ€è¿‘14å¤©)...")
    cols = ["date", "f1", "f2", "f3", "f4", "f5", "f6", "predict", "actual", "bias"]
    df_log = pd.DataFrame(columns=cols)

    for i in range(14, 0, -1):
        d = today - timedelta(days=i)
        if not is_workday(d): continue
        
        # å°è¯•è®¡ç®—æ¯ä¸€å¤©
        # åªè¦æ˜¯å·¥ä½œæ—¥éƒ½ç®—ï¼Œä¸ºäº†ç”»å›¾å¥½çœ‹ï¼Œä¸ä»…ä»…æ˜¯æœ‰å®é™…å€¼æ‰ç®—
        fs = calculate_factors(d, df_p, df_fut, df_val, df_bond, df_margin)
        p_raw = round(sum(fs) / 6, 2)
        
        act = get_actual_val(d)
        bias = round(act - p_raw, 2) if act is not None else np.nan
        
        df_log.loc[len(df_log)] = [d.strftime("%Y-%m-%d")] + fs + [p_raw, act, bias]

    # --- 4. åŠ¨æ€æƒé‡ä¼˜åŒ– ---
    weights = np.array([1/6] * 6)
    df_fit = df_log.dropna(subset=['actual']).tail(10) # å–æœ€è¿‘10ä¸ªæœ‰æ•ˆæ•°æ®
    if len(df_fit) >= 5:
        X = df_fit[['f1', 'f2', 'f3', 'f4', 'f5', 'f6']].values
        y = df_fit['actual'].values
        # çº¦æŸï¼šæƒé‡å’Œä¸º1ï¼Œå•é¡¹æƒé‡åœ¨ 5% åˆ° 40% ä¹‹é—´
        res = minimize(lambda w: np.sum((X @ w - y)**2), weights, 
                       bounds=[(0.05, 0.4)]*6, 
                       constraints={'type':'eq','fun':lambda w: sum(w)-1})
        if res.success: weights = res.x

    # --- 5. ä»Šæ—¥æœ€ç»ˆè®¡ç®— ---
    today_fs = calculate_factors(today, df_p, df_fut, df_val, df_bond, df_margin)
    today_raw = round(sum(f * w for f, w in zip(today_fs, weights)), 2)
    
    # è¯¯å·®ä¿®æ­£
    bias_fix = 0.0
    if not df_fit.empty:
        # ä½¿ç”¨æŒ‡æ•°ç§»åŠ¨å¹³å‡æ¥å¹³æ»‘è¯¯å·®
        last_biases = df_fit['bias'].ewm(alpha=0.5).mean()
        bias_fix = last_biases.iloc[-1]
        if np.isnan(bias_fix): bias_fix = 0.0

    final_predict = round(today_raw + bias_fix, 2)

    # å­˜å…¥
    t_str = today.strftime("%Y-%m-%d")
    # å¦‚æœä»Šå¤©å·²ç»ç®—è¿‡ï¼ˆæ¯”å¦‚é‡è·‘ï¼‰ï¼Œå…ˆåˆ æ‰æ—§çš„
    df_log = df_log[df_log['date'] != t_str]
    df_log.loc[len(df_log)] = [t_str] + today_fs + [today_raw, np.nan, np.nan]
    
    # ä¿å­˜
    df_log.sort_values('date').to_csv(LOG_FILE, index=False)
    log(f"è®¡ç®—å®Œæˆã€‚ä»Šæ—¥é¢„æµ‹: {final_predict} (æƒé‡ä¼˜åŒ–å)")

    # --- 6. é£ä¹¦æ¨é€ ---
    w_info = " | ".join([f"{n}:{w:.0%}" for n, w in zip(["æ³¢åŠ¨","é‡èƒ½","å¼ºåº¦","æœŸç°","è‚¡å€º","æ æ†"], weights)])
    # æ„é€ é¢œè‰²ï¼šè´ªå©ª(>80)çº¢ï¼Œææƒ§(<20)ç»¿
    color_template = "red" if final_predict > 80 else ("green" if final_predict < 20 else "purple")
    
    payload = {
        "msg_type": "interactive",
        "card": {
            "header": {
                "title": {"tag": "plain_text", "content": f"ğŸ¯ æè´ª AI å®ç›˜é¢„æµ‹ ({today})"}, 
                "template": color_template
            },
            "elements": [
                {
                    "tag": "div", 
                    "text": {
                        "tag": "lark_md", 
                        "content": f"**ä»Šæ—¥å»ºè®®å€¼ï¼š{final_predict}**\n"
                                   f"åŸç”Ÿåˆ†ï¼š{today_raw} | ä¿®æ­£ï¼š{bias_fix:+.1f}\n"
                                   f"----------------\n"
                                   f"ğŸ“Š **çœŸå®æ•°æ®å› å­ï¼š**\n"
                                   f"ğŸŒŠ æ³¢åŠ¨: {today_fs[0]} | ğŸ”‹ é‡èƒ½: {today_fs[1]}\n"
                                   f"ğŸ’ª å¼ºåº¦: {today_fs[2]} | âš–ï¸ æœŸç°: {today_fs[3]}\n"
                                   f"ğŸ›¡ï¸ è‚¡å€º: {today_fs[4]} | ğŸ° æ æ†: {today_fs[5]}\n"
                                   f"----------------\n"
                                   f"ğŸ§  **AI æƒé‡è¿›åŒ–ï¼š**\n{w_info}"
                    }
                },
                {"tag": "note", "elements": [{"tag": "plain_text", "content": "æ³¨ï¼šæ•°æ®æºå·²åˆ‡æ¢ä¸º AkShare å®æ—¶æ¥å£ | å…³é”®è¯: æè´ª"}]}
            ]
        }
    }
    
    if FEISHU_WEBHOOK:
        try:
            r = requests.post(FEISHU_WEBHOOK, json=payload)
            log(f"æ¨é€çŠ¶æ€: {r.status_code}")
        except Exception as e:
            log(f"æ¨é€å¤±è´¥: {e}")

if __name__ == "__main__":
    main()