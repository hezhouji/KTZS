import akshare as ak
import pandas as pd
import numpy as np
from scipy import stats
import requests
import os
from datetime import datetime, timedelta

# é…ç½®
FEISHU_WEBHOOK = os.getenv("FEISHU_WEBHOOK")
DATA_DIR = "KTZS"
LOG_FILE = "HISTORY_LOG.csv"

def log(msg):
    print(f"[{datetime.now().strftime('%H:%M:%S')}] {msg}")

def is_workday(date):
    """ç®€å•åˆ¤æ–­æ˜¯å¦ä¸ºå·¥ä½œæ—¥ï¼ˆè·³è¿‡å‘¨å…­æ—¥ï¼‰"""
    return date.weekday() < 5

def get_target_dates(today):
    """
    é€»è¾‘ï¼šå¦‚æœæ˜¯å‘¨ä¸€ï¼Œæˆ‘ä»¬éœ€è¦å‰ä¸€å¤©çš„æ•°æ®ï¼ˆä¸Šå‘¨äº”ï¼‰
    å¦‚æœæ˜¯å‘¨äºŒåˆ°å‘¨äº”ï¼Œæˆ‘ä»¬éœ€è¦å‰ä¸€å¤©ï¼ˆå‘¨ä¸€åˆ°å‘¨å››ï¼‰
    """
    yest = today - timedelta(days=1)
    while not is_workday(yest):
        yest -= timedelta(days=1)
    return yest

def get_actual_val(date_obj):
    date_str = date_obj.strftime("%Y%m%d")
    path = os.path.join(DATA_DIR, f"{date_str}.txt")
    if os.path.exists(path):
        try:
            with open(path, "r") as f:
                return float(f.read().strip())
        except: return None
    return None

def save_to_history(date_str, raw, bias, final):
    if not os.path.exists(LOG_FILE):
        with open(LOG_FILE, "w") as f:
            f.write("date,raw_score,bias,final_prediction\n")
    
    df = pd.read_csv(LOG_FILE)
    if str(date_str) not in df['date'].values.astype(str):
        with open(LOG_FILE, "a") as f:
            f.write(f"{date_str},{raw:.2f},{bias:.2f},{final:.2f}\n")
        log(f"âœ… å·²å­˜è¯ {date_str}")

def send_feishu(title, text, color="blue"):
    if not FEISHU_WEBHOOK: return
    payload = {
        "msg_type": "interactive",
        "card": {
            "header": {"title": {"tag": "plain_text", "content": title}, "template": color},
            "elements": [{"tag": "div", "text": {"tag": "lark_md", "content": text}}]
        }
    }
    requests.post(FEISHU_WEBHOOK, json=payload)

def calculate_score(target_date, df_p, df_val, df_bond):
    """æ ¸å¿ƒè®¡ç®—æ¨¡å‹ - å¢å¼ºå®¹é”™ç‰ˆ"""
    try:
        # æˆªå–æ•°æ®å¹¶è¿‡æ»¤ç©ºå€¼
        df_curr = df_p[df_p['date'] <= target_date].dropna(subset=['close', 'volume']).copy()
        if df_curr.empty: return None
        
        # 1. åŠ¨èƒ½ (å¤„ç†å¯èƒ½çš„ç©ºå€¼)
        h250 = df_curr['close'].rolling(250, min_periods=1).max()
        curr_ratio = df_curr['close'].iloc[-1] / h250.iloc[-1]
        s_score = stats.percentileofscore((df_curr['close']/h250).dropna(), curr_ratio)
        
        # 2. é‡èƒ½
        v20 = df_curr['volume'].rolling(20, min_periods=1).mean()
        curr_v_ratio = df_curr['volume'].iloc[-1] / v20.iloc[-1]
        v_score = stats.percentileofscore((df_curr['volume']/v20).dropna(), curr_v_ratio)
        
        # 3. è‚¡å€º (ERP) - å®¹æ˜“å‡º nan çš„é‡ç¾åŒº
        pe_col = 'å¸‚ç›ˆç‡1' if 'å¸‚ç›ˆç‡1' in df_val.columns else 'å¸‚ç›ˆç‡TTM'
        # å¼ºåˆ¶è½¬æ¢ç±»å‹å¹¶å¤„ç†ç©ºå€¼
        df_val[pe_col] = pd.to_numeric(df_val[pe_col], errors='coerce')
        df_bond['ä¸­å›½å›½å€ºæ”¶ç›Šç‡10å¹´'] = pd.to_numeric(df_bond['ä¸­å›½å›½å€ºæ”¶ç›Šç‡10å¹´'], errors='coerce')
        
        merged = pd.merge(df_val[['date_key', pe_col]], df_bond[['date_key', 'ä¸­å›½å›½å€ºæ”¶ç›Šç‡10å¹´']], on='date_key').dropna()
        merged = merged[merged['date_key'] <= target_date]
        
        if not merged.empty:
            merged['erp'] = (1 / merged[pe_col]) - (merged['ä¸­å›½å›½å€ºæ”¶ç›Šç‡10å¹´'] / 100)
            e_score = 100 - stats.percentileofscore(merged['erp'], merged['erp'].iloc[-1])
        else:
            log("ERPæ•°æ®åŒ¹é…ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤åˆ†50")
            e_score = 50

        # åŠ æƒè®¡ç®—ï¼Œå¹¶ä½¿ç”¨ np.nan_to_num å…œåº•
        raw = (np.nan_to_num(s_score) * 0.4 + 
               np.nan_to_num(v_score) * 0.3 + 
               np.nan_to_num(e_score) * 0.3)
        
        return round(raw, 2)
    except Exception as e:
        log(f"å› å­è®¡ç®—å´©è£‚: {e}")
        return None
def main():
    log("=== å¯åŠ¨å…·å¤‡å‘¨æœ«æ„ŸçŸ¥èƒ½åŠ›çš„åˆ†ææµç¨‹ ===")
    today = datetime.now().date()
    if not is_workday(today):
        log("ä»Šæ—¥éäº¤æ˜“æ—¥ï¼Œè·³è¿‡ã€‚")
        return

    # 1. æ‹‰å–æ•°æ®
    log("åŒæ­¥å¸‚åœºæ•°æ®...")
    df_p = ak.stock_zh_index_daily(symbol="sh000300")
    df_p['date'] = pd.to_datetime(df_p['date']).dt.date
    df_val = ak.stock_zh_index_value_csindex(symbol="000300")
    df_val['date_key'] = pd.to_datetime(df_val['æ—¥æœŸ']).dt.date
    df_bond = ak.bond_zh_us_rate()
    df_bond['date_key'] = pd.to_datetime(df_bond['æ—¥æœŸ']).dt.date

    # 2. è¡¥ç®—ä¸æŠ¥è­¦é€»è¾‘
    # æ£€æŸ¥è¿‡å»5ä¸ªå·¥ä½œæ—¥æ˜¯å¦æœ‰å¾…è¡¥å½•æ•°æ®
    last_bias = 0
    for i in range(5, 0, -1):
        check_day = today - timedelta(days=i)
        if not is_workday(check_day): continue
        
        actual = get_actual_val(check_day)
        raw = calculate_score(check_day, df_p, df_val, df_bond)
        
        if actual and raw:
            bias = actual - raw
            save_to_history(check_day.strftime("%Y-%m-%d"), raw, bias, actual)
            last_bias = bias # è®°å½•æœ€è¿‘ä¸€æ¬¡çš„æœ‰æ•ˆåå·®
        elif i == 1 or (today.weekday() == 0 and (today - check_day).days <= 3):
            # å¦‚æœæ˜¯â€œä¸Šä¸€å·¥ä½œæ—¥â€ç¼ºå¤±ï¼Œå‘é£ä¹¦é€šçŸ¥
            if not actual:
                send_feishu("âš ï¸ æè´ªæŒ‡æ•°è¡¥å½•æé†’", f"ç¼ºå¤±æ—¥æœŸ: **{check_day}**\nè¯·åœ¨ `KTZS/` æ–‡ä»¶å¤¹è¡¥ä¸Šä¼ è¯¥æ—¥æ•°å€¼æ–‡ä»¶ã€‚", "orange")

    # 3. ä»Šæ—¥é¢„æµ‹
    today_raw = calculate_score(today, df_p, df_val, df_bond)
    if today_raw:
        # å¦‚æœå†å²è®°å½•é‡Œæœ‰æœ€è¿‘çš„åå·®ï¼Œç›´æ¥ä½¿ç”¨
        if os.path.exists(LOG_FILE):
            df_h = pd.read_csv(LOG_FILE)
            if not df_h.empty: last_bias = df_h['bias'].iloc[-1]
            
        final = round(today_raw + last_bias, 2)
        send_feishu(f"ğŸ“Š æè´ªæŒ‡æ•°é¢„æµ‹ ({today})", 
                    f"**é¢„æµ‹å€¼ï¼š{final}**\nåŸç”Ÿï¼š{today_raw} | ä¿®æ­£ï¼š{last_bias:+.2f}\n\n*æ³¨ï¼šå·²è‡ªåŠ¨å¯¹é½ä¸Šä¸€å·¥ä½œæ—¥è¯¯å·®ã€‚*", "blue")

if __name__ == "__main__":
    main()